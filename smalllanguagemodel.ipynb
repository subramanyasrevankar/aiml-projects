{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNERZoOxv6s8M6FoARts1YR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d3e758372344194a783de3ea6cb08e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28d8922d06d34fd9a51e34d0f1084c62",
              "IPY_MODEL_19b43e8c95804a26813da2359c3440b1",
              "IPY_MODEL_711a16536e234ddc96d1cfa2a536cfe5"
            ],
            "layout": "IPY_MODEL_416c3299168440b0a33649de77e69a61"
          }
        },
        "28d8922d06d34fd9a51e34d0f1084c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_854417cba89a4264a17681789a7b5ba1",
            "placeholder": "​",
            "style": "IPY_MODEL_3a76e3fa77304733a1873f2fea8fec26",
            "value": "  0%"
          }
        },
        "19b43e8c95804a26813da2359c3440b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51e847a6bbc04bfc99e15d0bcb2c3529",
            "max": 20000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bfc0d3123d8d4612a515d46a1b63ac4f",
            "value": 141
          }
        },
        "711a16536e234ddc96d1cfa2a536cfe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aa8d00091334e65b556787edf26ffff",
            "placeholder": "​",
            "style": "IPY_MODEL_b627510b2eb44622a34dc1083c7d519a",
            "value": " 141/20000 [39:24&lt;92:13:38, 16.72s/it]"
          }
        },
        "416c3299168440b0a33649de77e69a61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "854417cba89a4264a17681789a7b5ba1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a76e3fa77304733a1873f2fea8fec26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51e847a6bbc04bfc99e15d0bcb2c3529": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfc0d3123d8d4612a515d46a1b63ac4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8aa8d00091334e65b556787edf26ffff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b627510b2eb44622a34dc1083c7d519a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subramanyasrevankar/aiml-projects/blob/main/smalllanguagemodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction** :\n",
        "lets us build a small Language Model(SLM) from scratch .we will try to keep the parameter size to 10-15 million. Our goal is to generate creative and coherent text based on the input data."
      ],
      "metadata": {
        "id": "z1UcluJBeypJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1:Import the Dataset\n",
        "Tinystories is a synthetic dataset of short stories that only contain words that a typical 3 to 4 year olds usually generated by GPT 3.5 and gpt 4 .we can get it from HuggingFace"
      ],
      "metadata": {
        "id": "v5D6bmmkfmKS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FmMfyYoTq5X"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NK4zUbFYgTaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('roneneldan/TinyStories')"
      ],
      "metadata": {
        "id": "iFsN_FWzUztL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2:Tokenize the Dataset\n",
        "in this step we will do the following\n",
        "1)tokenise the dataset into tokenIDs\n",
        "2)create a file called \"train.bin\" and \"validation.bin\" where we store the tokenIDs from the entire data\n",
        "3)we make sure the tokenIDs are stored on a disk ,rather than on the RAM for efficient computations."
      ],
      "metadata": {
        "id": "6OAdLSojgVtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "enc=tiktoken.get_encoding(\"gpt2\") #tiktoken is a library from where we are getting the tokenizer\n",
        "\n",
        "def process(example):\n",
        "  ids=enc.encode_ordinary(example['text']) #encode_ordinary ignores  any special tokens\n",
        "  out={'ids':ids,'len':len(ids)}\n",
        "  return out\n",
        "\n",
        "if not os.path.exists('train.bin'):\n",
        "    tokenized=dataset.map(\n",
        "        process,remove_columns=['text'],\n",
        "        desc='tokenizing the splits',\n",
        "        # num_proc=8, # Removed num_proc for better compatibility in Colab\n",
        "    )\n",
        "#concatinate all the ids in each dataset into one large file we can use for training\n",
        "for split,dset in tokenized.items():\n",
        "  arr_len=np.sum(dset['len'])\n",
        "  filename=f'{split}.bin'\n",
        "  dtype=np.uint16\n",
        "  arr=np.memmap(filename,dtype=dtype,mode='w+',shape=(arr_len,))\n",
        "  total_batches=1024\n",
        "  idx=0\n",
        "  for batch_idx in tqdm(range(total_batches),desc=f'writing{filename}'):\n",
        "    batch=dset.shard(num_shards=total_batches,index=batch_idx,contiguous=True).with_format('numpy')\n",
        "    arr_batch=np.concatenate(batch['ids'])\n",
        "    arr[idx:idx+len(arr_batch)]=arr_batch\n",
        "    idx+=len(arr_batch)\n",
        "  arr.flush() #it helps to write it in the disk"
      ],
      "metadata": {
        "id": "ayA6uHRShd6r",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3:Create Input-Output batches for the dataset\n"
      ],
      "metadata": {
        "id": "VFvt6Hpy4-bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split): #it creates a getbatch which consists of two matrices input and the output matrices\n",
        "  if split=='train':\n",
        "    data=np.memmap('train.bin',dtype=np.uint16,mode='r')\n",
        "  else:\n",
        "    data=np.memmap('validation.bin',dtype=np.uint16,mode='r')\n",
        "  ix=torch.randint(len(data)-block_size,(batch_size,))   #\n",
        "  x=torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "  y=torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "  if device_type=='cuda':  #the pin arrays x,y which allows move them to GPU asynchronously (non_blocking=True)\n",
        "    x,y=x.pin_memory().to(device,non_blocking=True),y.pin_memory().to(device,non_blocking=True)\n",
        "  else:\n",
        "    x,y=x.to(device),y.to(device)\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "Cq8ZrHR25Io4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4:Create the SLM model\n"
      ],
      "metadata": {
        "id": "SkDIcmYhTPpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from contextlib import nullcontext\n",
        "import os\n",
        "#block size is equal to the context size\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "    def forward(self, x):\n",
        "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                       .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        if self.flash:\n",
        "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int\n",
        "    vocab_size: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_embd: int\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop=nn.Dropout(config.dropout),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "            return logits, loss\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            return logits, None\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Generate tokens given a conditioning sequence.\n",
        "        idx: Tensor of shape (B, T)\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "\n",
        "#layer normalization in here ->converting it into the high dimensional vector->768 (embedding vector) [.....]#after this the main layer is passed into the masked multihead attention which also has the same shape (768) but it has the context(it understands what the language is)->attention score is the matrix f 4*4 matrix ->three matrices query(4*768),key(4*768),value(4*768)->attention score matrix (Q*k transpose) of 4*4->after it converted into attention weight (root dkeys + softmax) matrix multiplied with the values (matrix) which give me the context vector (4*768)->causal attention(the elements above the diagonal are zero).Transformer Block which consists of layerNorm 1->Multi-head attention->Dropout->+->Layer Norm2->Feed forward NN->dropout (more than 12 transformer block)->shape will be same (768)->then the neural network(outputs)->batch_size*4*50257->every word->corresponding to the vocabulary size token id it should match the id.->logits matrix->4*50257\n",
        "#Gradient Accumulation:You want to train with a batchsize of 1024->but your GPU only fits the batch size of 32 then at first it runs the 32 iterations and after that it updates the parameters it doesn't wait until 1024 after every 32 steps it updates the parameters every 32 steps (gradient accumulation is possible only if the batch size is too large)"
      ],
      "metadata": {
        "id": "AZGX4Pj0pQLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPTConfig(\n",
        "    vocab_size=50257,     # use the tokenizer's vocab size\n",
        "    block_size=128,       # or whatever context size you're training with\n",
        "    n_layer=6,\n",
        "    n_head=6,\n",
        "    n_embd=384,\n",
        "    dropout=0.1,\n",
        "    bias=True\n",
        ")\n",
        "\n",
        "model = GPT(config)"
      ],
      "metadata": {
        "id": "-KP6bvor3tUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5:Define the Loss function\n"
      ],
      "metadata": {
        "id": "2JL30pYS54Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = get_batch(split)\n",
        "                with ctx:\n",
        "                    logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "DwgOVD8H6FGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6:Define the SLM training configuration Part 1\n"
      ],
      "metadata": {
        "id": "_RuCiHYU6HwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Config\n",
        "import torch\n",
        "from contextlib import nullcontext\n",
        "\n",
        "learning_rate = 1e-4 #more stable training, earlier 1e-4\n",
        "max_iters = 20000 #increase from 25000\n",
        "warmup_steps = 1000 #smoother initial train, earlier 100\n",
        "min_lr = 5e-4 #lower rate, earlier 5e-4\n",
        "eval_iters = 500 # increased from 100\n",
        "batch_size = 32 # changed from 16, better gradient estimate\n",
        "block_size = 128 #changed from 64, capture longer range dependencies\n",
        "\n",
        "gradient_accumulation_steps = 32 # reduced from 50\n",
        "\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "\n",
        "# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n",
        "#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "torch.set_default_device(device)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "8GXPVxCM6mdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7:Define SLM training COnfiguration Part 2\n"
      ],
      "metadata": {
        "id": "gigmN7Dt6qQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
        "\n",
        "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
        "\n",
        "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
        "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
        "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
        "\n",
        "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ],
      "metadata": {
        "id": "Um_OJh166y5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8:Pre-train the SLM"
      ],
      "metadata": {
        "id": "X2C5OBFp62vI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pre train the model:\n",
        "for each iteration choose x and y .\n",
        "\n",
        "pass x through the model to get the logits .\n",
        "\n",
        "Compute the loss between my logits and target.(cross entropy)\n",
        "\n",
        "backpropogate the loss\n",
        "\n",
        "Accumulate the gradients till we reach \"gradient accumulation\"\n",
        "\n",
        "Update the parameters\n",
        "\n",
        "Update learning rate\n",
        "\n",
        "Evaluate and save test\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yJeydbog24NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "# Ensure model is on the correct device\n",
        "model = model.to(device)\n",
        "\n",
        "# In your training loop\n",
        "for epoch in tqdm(range(max_iters)):\n",
        "    if epoch % eval_iters == 0 and epoch != 0:\n",
        "        # Ensure estimate_loss uses the correct device\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
        "        train_loss_list += [losses['train']]\n",
        "        validation_loss_list += [losses['val']]\n",
        "\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "    # Ensure X and y are on the correct device\n",
        "    X, y = get_batch(\"train\")\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    with ctx:\n",
        "        logits, loss = model(X, y)\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "kuEFhM7t6-rE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "4d3e758372344194a783de3ea6cb08e6",
            "28d8922d06d34fd9a51e34d0f1084c62",
            "19b43e8c95804a26813da2359c3440b1",
            "711a16536e234ddc96d1cfa2a536cfe5",
            "416c3299168440b0a33649de77e69a61",
            "854417cba89a4264a17681789a7b5ba1",
            "3a76e3fa77304733a1873f2fea8fec26",
            "51e847a6bbc04bfc99e15d0bcb2c3529",
            "bfc0d3123d8d4612a515d46a1b63ac4f",
            "8aa8d00091334e65b556787edf26ffff",
            "b627510b2eb44622a34dc1083c7d519a"
          ]
        },
        "outputId": "85d3b430-91a0-433d-a2ae-5ee8d5fda315"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d3e758372344194a783de3ea6cb08e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9:Plot the SLM Loss"
      ],
      "metadata": {
        "id": "U-8lHdQn7FZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
        "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
        "\n",
        "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
        "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
        "plt.xlabel(\"Steps - Every 100 epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8h5G2Pol7Ks9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10:Run the SLM inference on our trained model\n"
      ],
      "metadata": {
        "id": "0q4oz7et7SCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the model\n",
        "model = GPT(config)  # re-create the model with same config\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
      ],
      "metadata": {
        "id": "mGcLNFMe7ZlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Once upon a time there was a pumpkin.\"\n",
        "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
        "y = model.generate(context, 200)\n",
        "print(enc.decode(y.squeeze().tolist()))"
      ],
      "metadata": {
        "id": "_quwBINg7b1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"A little girl went to the woods\"\n",
        "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
        "y = model.generate(context, 200)\n",
        "print(enc.decode(y.squeeze().tolist()))"
      ],
      "metadata": {
        "id": "YfhnD34e7gCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "uHp3G-ck7lYf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}